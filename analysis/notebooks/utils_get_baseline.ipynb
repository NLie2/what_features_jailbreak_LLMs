{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_paths import (\n",
    "    PROJECT_ROOT,\n",
    "    DATASETS_DIR,\n",
    "    ANALYSIS_DIR,\n",
    "    EXPERIMENTS_DIR,\n",
    "    IMAGES_DIR,\n",
    "    GEN_DATA_DIR,\n",
    "    LOGS_DIR,\n",
    "    get_project_path\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os \n",
    "import argparse\n",
    "\n",
    "\n",
    "from analysis.supervised import *\n",
    "from analysis.causal_interventions import *\n",
    "from analysis.Probes import SimpleMLP\n",
    "from gen_data.harmBench_autograder import *\n",
    "from analysis.models import *\n",
    "\n",
    "\n",
    "\n",
    "def grade(autograder, autogr_tokenizer, df, request_column, column_to_grade, output_path):  \n",
    "  # Apply the grading function and add the ratings column\n",
    "  df[f\"rating_{column_to_grade}\"] = df.apply(lambda row: grade_with_HF(row[request_column], [row[column_to_grade]], autograder, autogr_tokenizer, verbose = True)[0], axis=1)\n",
    "  print(\"rated\")\n",
    "  # Save the dataframe to the output path\n",
    "  df.to_pickle(output_path)\n",
    "  print(output_path)\n",
    "  return df\n",
    "\n",
    "\n",
    "\n",
    "def get_baseline(model, model_name, tokenizer, harmful, autograder, autogr_tokenizer, results_csv_path, single = \"\"):\n",
    "    print(\"get baseline...\")\n",
    "    \n",
    "    # Create the directory path\n",
    "    base_path = Path(f\"/data/nathalie_maria_kirch/ERA_Fellowship/datasets/{model_name}/final/harmful_{single}/\")\n",
    "    base_path.mkdir(parents=True, exist_ok=True) \n",
    "  \n",
    "    harmful_map = {\n",
    "    \"harmful\": 0,\n",
    "    \"harmless\": 1,\n",
    "    \"haize\": 2\n",
    "    }\n",
    "    harmful_v = harmful_map[harmful]\n",
    "        \n",
    "    validation_path = [\n",
    "        \"/data/nathalie_maria_kirch/ERA_Fellowship/datasets/validation/validation_data.pkl\",\n",
    "        \"/data/nathalie_maria_kirch/ERA_Fellowship/datasets/validation/validation_data_benign.pkl\", \n",
    "        \"/data/nathalie_maria_kirch/ERA_Fellowship/datasets/haize/jailbreak_prompts_nathalie_cas_haize_1_short.pkl\"\n",
    "    ][harmful_v]\n",
    "    \n",
    "    df = pd.read_pickle(validation_path)\n",
    "  \n",
    "    df[\"control_no_intervention_response\"] = None  \n",
    "\n",
    "    # Run on every element of the sample\n",
    "    \n",
    "    model.cuda()\n",
    "    for index, row in df.iterrows():\n",
    "      \n",
    "      prompt = row[\"jailbreak_prompt_text\"]\n",
    "      print(\"prompt: \", prompt)\n",
    "      \n",
    "      input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "      outputs = model.generate(**input_ids, max_new_tokens=200)\n",
    "\n",
    "      response = tokenizer.decode(outputs[0])\n",
    "      \n",
    "      \n",
    "      if response.startswith(prompt): response = response[len(prompt):].strip()\n",
    "      if response.startswith(\"<|begin_of_text|>\"+prompt): response = response[len(\"<|begin_of_text|>\"+prompt):].strip()\n",
    "      if response.startswith(\"<bos>\"+prompt): response = response[len(\"<bos>\"+prompt):].strip()\n",
    "\n",
    "      df.at[index, \"control_no_intervention_response\"] = response\n",
    "      print(response)\n",
    "\n",
    "      # Save periodically\n",
    "      if index % 20 == 0:\n",
    "        folder_path = Path(f\"/data/nathalie_maria_kirch/ERA_Fellowship/datasets/{model_name}/final/{harmful}/\")\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        df.to_pickle(folder_path / f\"no_intervention_control.pkl\")\n",
    "    \n",
    "    df.to_pickle(folder_path / f\"no_intervention_control.pkl\")\n",
    "    print(f\"Data saved to {str(folder_path / f\"no_intervention_control.pkl\")}\")\n",
    "    model.cpu()\n",
    "    \n",
    "    autograder.cuda()\n",
    "    \n",
    "    results_df = grade(\n",
    "      autograder=autograder,\n",
    "      autogr_tokenizer=autogr_tokenizer,\n",
    "      df=df,\n",
    "      request_column=\"jailbreak_prompt_text\",\n",
    "      column_to_grade=f\"control_no_intervention_response\",\n",
    "      output_path= f\"/data/nathalie_maria_kirch/ERA_Fellowship/datasets/{model_name}/final/harmful/no_intervention_control_rated.pkl\"\n",
    "      )\n",
    "    autograder.cpu()\n",
    "    \n",
    "                    \n",
    "    # Calculate ASR\n",
    "    rating_col = f\"rating_control_no_intervention_response\"\n",
    "    asr = results_df[rating_col].value_counts().get(\"Yes\", 0)\n",
    "    print(f\"ASR: {asr}\")\n",
    "    \n",
    "    # Create results dictionary\n",
    "    new_result = {\n",
    "        \"asr\": asr,\n",
    "        \"model\": model_name,\n",
    "        \"probe_type\": \"control\",\n",
    "        \"offensive_defensive\": None,\n",
    "        \"c_strength\": None,\n",
    "        \"lr\": None\n",
    "    }\n",
    "\n",
    "    # Handle CSV operations\n",
    "    if results_csv_path.exists():\n",
    "        existing_df = pd.read_csv(results_csv_path)\n",
    "        # Convert the new result to a single-row DataFrame\n",
    "        new_result_df = pd.DataFrame([new_result])\n",
    "        # Concatenate with existing results\n",
    "        combined_df = pd.concat([existing_df, new_result_df], ignore_index=True)\n",
    "    else:\n",
    "        # Create new DataFrame with just the new result\n",
    "        combined_df = pd.DataFrame([new_result])\n",
    "\n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(results_csv_path, index=False)\n",
    "    print(f\"Updated results saved to {results_csv_path}\")\n",
    "    \n",
    "    return asr\n",
    "\n",
    "def main(model_name, single= \"control\"):   \n",
    "  model_name, num_layers, rating_column, hf_ident, input_size  =  get_model_details(model_name)\n",
    "\n",
    "  ## Get middle layer\n",
    "  middle_layer = num_layers // 2\n",
    "\n",
    "  ## CONFIG\n",
    "  model = AutoModelForCausalLM.from_pretrained(hf_ident)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(hf_ident)\n",
    "\n",
    "  autogr_name = \"cais/HarmBench-Llama-2-13b-cls\"\n",
    "  autograder = AutoModelForCausalLM.from_pretrained(autogr_name)\n",
    "  autogr_tokenizer = AutoTokenizer.from_pretrained(autogr_name, use_fast=False, truncation_side=\"left\", padding_side=\"left\")\n",
    "\n",
    "  get_baseline(model = model, model_name = model_name, tokenizer= tokenizer, harmful = \"harmful\", autograder = autograder, autogr_tokenizer = autogr_tokenizer, results_csv_path = DATASETS_DIR / f\"{model_name}/causal_intervention_parameters_{single}.csv\")\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"Llama-3.2-3B-Instruct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
