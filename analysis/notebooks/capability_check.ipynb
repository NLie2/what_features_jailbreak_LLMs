{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lm_eval in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (0.4.4)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (0.32.1)\n",
      "Requirement already satisfied: evaluate in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (2.20.0)\n",
      "Requirement already satisfied: jsonlines in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (4.0.0)\n",
      "Requirement already satisfied: numexpr in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (2.8.7)\n",
      "Requirement already satisfied: peft>=0.2.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (0.13.0)\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (2.13.6)\n",
      "Requirement already satisfied: pytablewriter in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (1.2.0)\n",
      "Requirement already satisfied: rouge-score>=0.0.4 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (0.1.2)\n",
      "Requirement already satisfied: sacrebleu>=1.5.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (2.4.3)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (1.4.2)\n",
      "Requirement already satisfied: sqlitedict in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (2.1.0)\n",
      "Requirement already satisfied: torch>=1.8 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (2.4.0)\n",
      "Requirement already satisfied: tqdm-multiprocess in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (0.0.11)\n",
      "Requirement already satisfied: transformers>=4.1 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (4.44.0)\n",
      "Requirement already satisfied: zstandard in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (0.22.0)\n",
      "Requirement already satisfied: dill in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (0.3.8)\n",
      "Requirement already satisfied: word2number in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (1.1)\n",
      "Requirement already satisfied: more-itertools in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from lm_eval) (10.1.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0->lm_eval) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0->lm_eval) (23.2)\n",
      "Requirement already satisfied: psutil in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0->lm_eval) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0->lm_eval) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0->lm_eval) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0->lm_eval) (0.4.2)\n",
      "Requirement already satisfied: filelock in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from datasets>=2.16.0->lm_eval) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from datasets>=2.16.0->lm_eval) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from datasets>=2.16.0->lm_eval) (0.6)\n",
      "Requirement already satisfied: pandas in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from datasets>=2.16.0->lm_eval) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from datasets>=2.16.0->lm_eval) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from datasets>=2.16.0->lm_eval) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from datasets>=2.16.0->lm_eval) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from datasets>=2.16.0->lm_eval) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0->lm_eval) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from datasets>=2.16.0->lm_eval) (3.9.5)\n",
      "Requirement already satisfied: absl-py in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from rouge-score>=0.0.4->lm_eval) (2.1.0)\n",
      "Requirement already satisfied: nltk in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from rouge-score>=0.0.4->lm_eval) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from rouge-score>=0.0.4->lm_eval) (1.16.0)\n",
      "Requirement already satisfied: portalocker in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm_eval) (2.10.1)\n",
      "Requirement already satisfied: regex in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm_eval) (2023.10.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm_eval) (0.9.0)\n",
      "Requirement already satisfied: colorama in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm_eval) (0.4.6)\n",
      "Requirement already satisfied: lxml in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm_eval) (5.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm_eval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm_eval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm_eval) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (4.11.0)\n",
      "Requirement already satisfied: sympy in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (1.12)\n",
      "Requirement already satisfied: networkx in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from torch>=1.8->lm_eval) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm_eval) (12.5.82)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from transformers>=4.1->lm_eval) (0.19.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from jsonlines->lm_eval) (23.1.0)\n",
      "Requirement already satisfied: DataProperty<2,>=1.0.1 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from pytablewriter->lm_eval) (1.0.1)\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from pytablewriter->lm_eval) (1.1.3)\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from pytablewriter->lm_eval) (3.2.1)\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from pytablewriter->lm_eval) (1.3.3)\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from pytablewriter->lm_eval) (0.1.6)\n",
      "Requirement already satisfied: typepy<2,>=1.3.2 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval) (1.3.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->lm_eval) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->lm_eval) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->lm_eval) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->lm_eval) (1.9.3)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval) (4.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.9 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval) (2024.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.8->lm_eval) (2.1.3)\n",
      "Requirement already satisfied: click in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from nltk->rouge-score>=0.0.4->lm_eval) (8.1.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->lm_eval) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /data/nathalie_maria_kirch/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.8->lm_eval) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 24 13:38:19 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:50:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             87W /  400W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 21:20:26.698254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-22 21:20:26.712576: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-22 21:20:26.716924: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-22 21:20:26.729484: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-22 21:20:28.812214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-10-22 21:20:32,569\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## CONFIG\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-7b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "## CONFIG\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move model to GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "model = HFLM(model)\n",
    "model.model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 216/216 [00:00<00:00, 848.21it/s]\n",
      "100%|████████| 203/203 [00:00<00:00, 849.53it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 845.79it/s]\n",
      "100%|████████| 152/152 [00:00<00:00, 854.57it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 844.35it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 841.79it/s]\n",
      "100%|████████| 145/145 [00:00<00:00, 843.40it/s]\n",
      "100%|████████| 151/151 [00:00<00:00, 851.22it/s]\n",
      "100%|████████| 112/112 [00:00<00:00, 851.96it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 855.98it/s]\n",
      "100%|████████| 135/135 [00:00<00:00, 857.38it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 851.84it/s]\n",
      "100%|████████| 144/144 [00:00<00:00, 857.89it/s]\n",
      "100%|████████| 310/310 [00:00<00:00, 849.98it/s]\n",
      "100%|████████| 235/235 [00:00<00:00, 844.91it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 849.84it/s]\n",
      "100%|████████| 270/270 [00:00<00:00, 851.30it/s]\n",
      "100%|████████| 102/102 [00:00<00:00, 852.32it/s]\n",
      "100%|████████| 378/378 [00:00<00:00, 851.20it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 849.28it/s]\n",
      "100%|████████| 234/234 [00:00<00:00, 819.12it/s]\n",
      "100%|████████| 306/306 [00:00<00:00, 803.03it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 827.70it/s]\n",
      "100%|████████| 265/265 [00:00<00:00, 813.44it/s]\n",
      "100%|████████| 103/103 [00:00<00:00, 820.13it/s]\n",
      "100%|████████| 783/783 [00:00<00:00, 806.06it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 798.55it/s]\n",
      "100%|████████| 223/223 [00:00<00:00, 732.46it/s]\n",
      "100%|████████| 272/272 [00:00<00:00, 822.75it/s]\n",
      "100%|████████| 166/166 [00:00<00:00, 832.25it/s]\n",
      "100%|████████| 282/282 [00:00<00:00, 817.74it/s]\n",
      "100%|████████| 173/173 [00:00<00:00, 819.73it/s]\n",
      "100%|████████| 110/110 [00:00<00:00, 826.37it/s]\n",
      "100%|████████| 201/201 [00:00<00:00, 833.91it/s]\n",
      "100%|████████| 193/193 [00:00<00:00, 821.14it/s]\n",
      "100%|████████| 612/612 [00:00<00:00, 816.68it/s]\n",
      "100%|████████| 545/545 [00:00<00:00, 848.85it/s]\n",
      "100%|████████| 114/114 [00:00<00:00, 848.78it/s]\n",
      "100%|████████| 198/198 [00:00<00:00, 845.32it/s]\n",
      "100%|████████| 245/245 [00:00<00:00, 849.52it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 849.05it/s]\n",
      "100%|████████| 131/131 [00:00<00:00, 850.04it/s]\n",
      "100%|████████| 238/238 [00:00<00:00, 847.43it/s]\n",
      "100%|████████| 390/390 [00:00<00:00, 848.18it/s]\n",
      "100%|████████| 895/895 [00:01<00:00, 841.88it/s]\n",
      "100%|████████| 237/237 [00:00<00:00, 849.69it/s]\n",
      "100%|████████| 108/108 [00:00<00:00, 851.13it/s]\n",
      "100%|████████| 346/346 [00:00<00:00, 852.58it/s]\n",
      "100%|████████| 204/204 [00:00<00:00, 853.93it/s]\n",
      "100%|████████| 324/324 [00:00<00:00, 857.74it/s]\n",
      "100%|████████| 163/163 [00:00<00:00, 856.47it/s]\n",
      "100%|████████| 171/171 [00:00<00:00, 860.88it/s]\n",
      "100%|████████| 121/121 [00:00<00:00, 856.00it/s]\n",
      "100%|██████| 1534/1534 [00:01<00:00, 848.57it/s]\n",
      "100%|████████| 165/165 [00:00<00:00, 849.31it/s]\n",
      "100%|████████| 126/126 [00:00<00:00, 848.52it/s]\n",
      "100%|████████| 311/311 [00:00<00:00, 855.00it/s]\n",
      "Running loglikelihood requests:  86%|▊| 48261/56"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://compute-permanent-node-506:30996/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "\n",
    "eval_output = evaluator.simple_evaluate(model=model, tasks=['mmlu'], device='cuda', verbosity='ERROR')\n",
    "\n",
    "result = eval_output['results']['mmlu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc,none': 0.506266913545079,\n",
       " 'acc_stderr,none': 0.004030016016196707,\n",
       " 'alias': 'mmlu'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['acc,none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_parameters(model):\n",
    "  return all(p.device.type == 'cuda' for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with Linear Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 13:20:38.331719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-26 13:20:38.344886: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-26 13:20:38.348857: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-26 13:20:38.359171: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-26 13:20:40.117684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-10-26 13:20:42,448\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "from analysis.unsupervised import *\n",
    "from analysis.supervised import *\n",
    "from gen_data.get_activations import *\n",
    "\n",
    "model_name = \"google/gemma-7b-it\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gen_data.get_activations import get_res_layers_to_enumerate\n",
    "\n",
    "\n",
    "def save_model_with_hooks(model, \n",
    "                 tokenizer,  \n",
    "                 probe,\n",
    "                 layers_to_intervene=None,  # Allow multiple layers\n",
    "                 intervention_strength=1,\n",
    "                 tokens=\"all\", add_to=\"prompt\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if layers_to_intervene is None:\n",
    "        layers_to_intervene = [17]  # Default to layer 17 if no layers specified\n",
    "\n",
    "    # print out devices \n",
    "    # add print statements to see what we are stuck on \n",
    "    # try running function while disabling hook \n",
    "\n",
    "    def linear_intervene(name):\n",
    "        def hook(model, input, output):\n",
    "            if add_to==\"prompt\":\n",
    "                if output[0].shape[1] != 1:\n",
    "                    if tokens==\"last\":\n",
    "                        output[0][:, -1, :] += intervention_strength * torch.tensor(probe.coef_[0]).to(device)\n",
    "                    else:\n",
    "                        output[0][:, :, :] += intervention_strength * torch.tensor(probe.coef_[0]).to(device)\n",
    "\n",
    "            else: \n",
    "                if tokens==\"last\":\n",
    "                    # [1, 8, 3072]\n",
    "                    output[0][:, -1, :] += intervention_strength * torch.tensor(probe.coef_[0]).to(device)\n",
    "                    # out = output[0][:, -1, :] +intervention_strength * torch.tensor(probe.coef_[0]).to(device)\n",
    "                # elif tokens==\"prompt\":\n",
    "                    # out = output[0][:, :len_prompt, :] + intervention_strength * torch.tensor(probe.coef_[0]).to(device)\n",
    "                else:\n",
    "                    output[0][:, :, :] += intervention_strength * torch.tensor(probe.coef_[0]).to(device)\n",
    "                    # out = output[0][:, :, :] + intervention_strength * torch.tensor(probe.coef_[0]).to(device) #third colon is token position\n",
    "            \n",
    "            return output\n",
    "        return hook\n",
    "\n",
    "    layers_to_enum = get_res_layers_to_enumerate(model)\n",
    "    hooks = []\n",
    "\n",
    "    # Register hooks for each specified layer\n",
    "    for layer_index in layers_to_intervene:\n",
    "        if layer_index < 0 or layer_index >= len(layers_to_enum):\n",
    "            raise ValueError(f\"Layer {layer_index} is out of bounds for the model.\")\n",
    "        hook_handle = model.model.layers[layer_index].register_forward_hook(linear_intervene(layer_index))\n",
    "        hooks.append(hook_handle)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "linear_probe_path = \"/datasets/Probes/linear_probe.pkl\"\n",
    "\n",
    "with open(linear_probe_path, 'rb') as f:\n",
    "      saved_log_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offensive Linear Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9603a6b3d574895aae804810acd29c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "intervention_strength = 0.25\n",
    "save_model_with_hooks(model=model, tokenizer=tokenizer, probe=saved_log_model, layers_to_intervene=[16], intervention_strength=intervention_strength, tokens=\"all\", add_to=\"all\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "hlfm_model = HFLM(model)\n",
    "\n",
    "check_all_parameters(hlfm_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 216/216 [00:00<00:00, 835.89it/s]\n",
      "100%|████████| 203/203 [00:00<00:00, 831.95it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 832.51it/s]\n",
      "100%|████████| 152/152 [00:00<00:00, 834.12it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 827.18it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 831.01it/s]\n",
      "100%|████████| 145/145 [00:00<00:00, 838.63it/s]\n",
      "100%|████████| 151/151 [00:00<00:00, 831.32it/s]\n",
      "100%|████████| 112/112 [00:00<00:00, 834.98it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 835.93it/s]\n",
      "100%|████████| 135/135 [00:00<00:00, 837.65it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 834.53it/s]\n",
      "100%|████████| 144/144 [00:00<00:00, 839.20it/s]\n",
      "100%|████████| 310/310 [00:00<00:00, 837.76it/s]\n",
      "100%|████████| 235/235 [00:00<00:00, 832.61it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 828.87it/s]\n",
      "100%|████████| 270/270 [00:00<00:00, 834.31it/s]\n",
      "100%|████████| 102/102 [00:00<00:00, 830.00it/s]\n",
      "100%|████████| 378/378 [00:00<00:00, 833.03it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 834.62it/s]\n",
      "100%|████████| 234/234 [00:00<00:00, 835.93it/s]\n",
      "100%|████████| 306/306 [00:00<00:00, 835.66it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 828.57it/s]\n",
      "100%|████████| 265/265 [00:00<00:00, 835.84it/s]\n",
      "100%|████████| 103/103 [00:00<00:00, 832.79it/s]\n",
      "100%|████████| 783/783 [00:00<00:00, 834.21it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 836.83it/s]\n",
      "100%|████████| 223/223 [00:00<00:00, 837.82it/s]\n",
      "100%|████████| 272/272 [00:00<00:00, 836.32it/s]\n",
      "100%|████████| 166/166 [00:00<00:00, 839.70it/s]\n",
      "100%|████████| 282/282 [00:00<00:00, 836.20it/s]\n",
      "100%|████████| 173/173 [00:00<00:00, 836.76it/s]\n",
      "100%|████████| 110/110 [00:00<00:00, 835.67it/s]\n",
      "100%|████████| 201/201 [00:00<00:00, 839.23it/s]\n",
      "100%|████████| 193/193 [00:00<00:00, 831.04it/s]\n",
      "100%|████████| 612/612 [00:00<00:00, 832.63it/s]\n",
      "100%|████████| 545/545 [00:00<00:00, 834.31it/s]\n",
      "100%|████████| 114/114 [00:00<00:00, 832.33it/s]\n",
      "100%|████████| 198/198 [00:00<00:00, 833.83it/s]\n",
      "100%|████████| 245/245 [00:00<00:00, 835.72it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 836.05it/s]\n",
      "100%|████████| 131/131 [00:00<00:00, 839.90it/s]\n",
      "100%|████████| 238/238 [00:00<00:00, 835.29it/s]\n",
      "100%|████████| 390/390 [00:00<00:00, 839.32it/s]\n",
      "100%|████████| 895/895 [00:01<00:00, 836.21it/s]\n",
      "100%|████████| 237/237 [00:00<00:00, 832.97it/s]\n",
      "100%|████████| 108/108 [00:00<00:00, 839.75it/s]\n",
      "100%|████████| 346/346 [00:00<00:00, 840.95it/s]\n",
      "100%|████████| 204/204 [00:00<00:00, 836.66it/s]\n",
      "100%|████████| 324/324 [00:00<00:00, 839.66it/s]\n",
      "100%|████████| 163/163 [00:00<00:00, 837.04it/s]\n",
      "100%|████████| 171/171 [00:00<00:00, 838.76it/s]\n",
      "100%|████████| 121/121 [00:00<00:00, 840.69it/s]\n",
      "100%|██████| 1534/1534 [00:01<00:00, 838.97it/s]\n",
      "100%|████████| 165/165 [00:00<00:00, 834.68it/s]\n",
      "100%|████████| 126/126 [00:00<00:00, 830.68it/s]\n",
      "100%|████████| 311/311 [00:00<00:00, 839.00it/s]\n",
      "Running loglikelihood requests: 100%|█| 56168/56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc,none': 0.5064805583250249,\n",
       " 'acc_stderr,none': 0.004030596967044771,\n",
       " 'alias': 'mmlu'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_output = evaluator.simple_evaluate(model=hlfm_model, tasks=['mmlu'], device='cuda', verbosity='ERROR')\n",
    "eval_output['results']['mmlu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval_output['results']['mmlu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "file_path = '/datasets/capabilities/capabilities.json'\n",
    "\n",
    "# Convert the numeric key to a string\n",
    "key_string = f\"linear {str(intervention_strength)}\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        existing_data = json.load(json_file)\n",
    "        \n",
    "        # If the key already exists, append the result to the list under that key\n",
    "        if key_string in existing_data:\n",
    "            existing_data[key_string].append(result)\n",
    "        else:\n",
    "            existing_data[key_string] = [result]  # Start a new list if the key doesn't exist\n",
    "else:\n",
    "    existing_data = {key_string: [result]}  # Create a new dict with the key and a list containing the result\n",
    "\n",
    "# Write back to the JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(existing_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defensive linear intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce491453f08483296433913a65d8ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "intervention_strength = -0.25\n",
    "save_model_with_hooks(model=model, tokenizer=tokenizer, probe=saved_log_model, layers_to_intervene=[16], intervention_strength=intervention_strength, tokens=\"all\", add_to=\"all\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "hlfm_model = HFLM(model)\n",
    "\n",
    "check_all_parameters(hlfm_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 216/216 [00:00<00:00, 795.97it/s]\n",
      "100%|████████| 203/203 [00:00<00:00, 786.26it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 773.92it/s]\n",
      "100%|████████| 152/152 [00:00<00:00, 771.75it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 766.01it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 778.14it/s]\n",
      "100%|████████| 145/145 [00:00<00:00, 780.46it/s]\n",
      "100%|████████| 151/151 [00:00<00:00, 778.65it/s]\n",
      "100%|████████| 112/112 [00:00<00:00, 783.31it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 773.06it/s]\n",
      "100%|████████| 135/135 [00:00<00:00, 787.74it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 755.02it/s]\n",
      "100%|████████| 144/144 [00:00<00:00, 772.10it/s]\n",
      "100%|████████| 310/310 [00:00<00:00, 785.97it/s]\n",
      "100%|████████| 235/235 [00:00<00:00, 788.34it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 768.55it/s]\n",
      "100%|████████| 270/270 [00:00<00:00, 772.05it/s]\n",
      "100%|████████| 102/102 [00:00<00:00, 774.94it/s]\n",
      "100%|████████| 378/378 [00:00<00:00, 793.61it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 776.07it/s]\n",
      "100%|████████| 234/234 [00:00<00:00, 790.30it/s]\n",
      "100%|████████| 306/306 [00:00<00:00, 780.98it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 771.66it/s]\n",
      "100%|████████| 265/265 [00:00<00:00, 783.24it/s]\n",
      "100%|████████| 103/103 [00:00<00:00, 785.27it/s]\n",
      "100%|████████| 783/783 [00:00<00:00, 789.01it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 779.84it/s]\n",
      "100%|████████| 223/223 [00:00<00:00, 780.83it/s]\n",
      "100%|████████| 272/272 [00:00<00:00, 783.93it/s]\n",
      "100%|████████| 166/166 [00:00<00:00, 779.76it/s]\n",
      "100%|████████| 282/282 [00:00<00:00, 785.94it/s]\n",
      "100%|████████| 173/173 [00:00<00:00, 788.11it/s]\n",
      "100%|████████| 110/110 [00:00<00:00, 776.82it/s]\n",
      "100%|████████| 201/201 [00:00<00:00, 784.85it/s]\n",
      "100%|████████| 193/193 [00:00<00:00, 776.49it/s]\n",
      "100%|████████| 612/612 [00:00<00:00, 801.05it/s]\n",
      "100%|████████| 545/545 [00:00<00:00, 801.67it/s]\n",
      "100%|████████| 114/114 [00:00<00:00, 789.26it/s]\n",
      "100%|████████| 198/198 [00:00<00:00, 801.38it/s]\n",
      "100%|████████| 245/245 [00:00<00:00, 809.65it/s]\n",
      "100%|████████| 100/100 [00:00<00:00, 788.81it/s]\n",
      "100%|████████| 131/131 [00:00<00:00, 789.51it/s]\n",
      "100%|████████| 238/238 [00:00<00:00, 806.73it/s]\n",
      "100%|████████| 390/390 [00:00<00:00, 808.76it/s]\n",
      "100%|████████| 895/895 [00:01<00:00, 811.60it/s]\n",
      "100%|████████| 237/237 [00:00<00:00, 806.05it/s]\n",
      "100%|████████| 108/108 [00:00<00:00, 804.47it/s]\n",
      "100%|████████| 346/346 [00:00<00:00, 805.11it/s]\n",
      "100%|████████| 204/204 [00:00<00:00, 805.72it/s]\n",
      "100%|████████| 324/324 [00:00<00:00, 811.00it/s]\n",
      "100%|████████| 163/163 [00:00<00:00, 804.38it/s]\n",
      "100%|████████| 171/171 [00:00<00:00, 791.47it/s]\n",
      "100%|████████| 121/121 [00:00<00:00, 801.81it/s]\n",
      "100%|██████| 1534/1534 [00:01<00:00, 818.53it/s]\n",
      "100%|████████| 165/165 [00:00<00:00, 802.65it/s]\n",
      "100%|████████| 126/126 [00:00<00:00, 810.01it/s]\n",
      "100%|████████| 311/311 [00:00<00:00, 810.55it/s]\n",
      "Running loglikelihood requests: 100%|█| 56168/56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc,none': 0.5053411194986469,\n",
       " 'acc_stderr,none': 0.004027970445943166,\n",
       " 'alias': 'mmlu'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_output = evaluator.simple_evaluate(model=hlfm_model, tasks=['mmlu'], device='cuda', verbosity='ERROR')\n",
    "eval_output['results']['mmlu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval_output['results']['mmlu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "file_path = '/datasets/capabilities/capabilities.json'\n",
    "\n",
    "# Convert the numeric key to a string\n",
    "key_string = f\"linear {str(intervention_strength)}\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        existing_data = json.load(json_file)\n",
    "        \n",
    "        # If the key already exists, append the result to the list under that key\n",
    "        if key_string in existing_data:\n",
    "            existing_data[key_string].append(result)\n",
    "        else:\n",
    "            existing_data[key_string] = [result]  # Start a new list if the key doesn't exist\n",
    "else:\n",
    "    existing_data = {key_string: [result]}  # Create a new dict with the key and a list containing the result\n",
    "\n",
    "# Write back to the JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(existing_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with MLP intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 11:36:31.346055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-28 11:36:31.384453: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-28 11:36:31.388480: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-28 11:36:31.420898: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-28 11:36:33.750277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-10-28 11:36:37,540\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from gen_data.get_activations import get_res_layers_to_enumerate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gen_data.get_activations import get_res_layers_to_enumerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "from torch.amp import autocast\n",
    "\n",
    "# def insert_mlp_hook(model, \n",
    "#                  mlp,\n",
    "#                  layers_to_intervene=None,  # Allow multiple layers\n",
    "#                  target = 0):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     if layers_to_intervene is None:\n",
    "#         layers_to_intervene = [17]  # Default to layer 17 if no layers specified\n",
    "\n",
    "#     # print out devices \n",
    "#     # add print statements to see what we are stuck on \n",
    "#     # try running function while disabling hook \n",
    "#     criterion = nn.MSELoss()\n",
    "#     def linear_intervene(name, target):\n",
    "#         def hook(model, input, output):\n",
    "#           if output[0].shape[1] != 1:\n",
    "#             # print(\"output shape\", output[0].shape) # this sohuld not be 1 at second position\n",
    "          \n",
    "#             with torch.enable_grad():\n",
    "                        \n",
    "#               # Forward pass\n",
    "#               feature_vector = output[0][:, :, :].requires_grad_().to(output[0].device)          \n",
    "#               # print(feature_vector.shape, feature_vector.requires_grad)  # Should print torch.Size([1, 768]) True\n",
    "              \n",
    "#               # ## check old predicted class\n",
    "#               # pred = mlp(feature_vector)\n",
    "\n",
    "#               # # Apply a sigmoid to convert it into a probability (between 0 and 1)\n",
    "#               # probability = torch.sigmoid(pred)\n",
    "\n",
    "#               # # Check which category it belongs to (class 0 or 1)\n",
    "#               # predicted_class = (probability > 0.5).float()\n",
    "#               # # print(predicted_class)\n",
    "\n",
    "#               # print(\"OLD Predicted class:\", predicted_class)\n",
    "              \n",
    "#               ### \n",
    "              \n",
    "                      \n",
    "#               # Define perturbation as a parameter or tensor that requires gradient\n",
    "#               perturbation = torch.zeros_like(feature_vector, requires_grad=True).to(feature_vector.device)\n",
    "              \n",
    "#               printed_once = True\n",
    "\n",
    "#               for i in range(32):\n",
    "#                 # Apply perturbation to the feature vector\n",
    "#                 perturbed_vector = feature_vector + perturbation\n",
    "\n",
    "#                 # Get output from the MLP\n",
    "#                 mlp_out = mlp(perturbed_vector)\n",
    "                \n",
    "                \n",
    "#                 target_out = torch.zeros_like(mlp_out)  # Default to zero, or choose an appropriate value\n",
    "\n",
    "#                 # Target tensor (ensure its shape matches mlp_out's shape)\n",
    "#                 # target = torch.zeros_like(mlp_out)  # Match the output size\n",
    "          \n",
    "#                 if target == 0:\n",
    "#                   target_out = torch.zeros_like(mlp_out).to(output[0].device) # Match the output size  \n",
    "#                   if printed_once: \n",
    "#                     # print(\"TARGET\", target_out)\n",
    "#                     printed_once = False\n",
    "                  \n",
    "#                 if target == 1:\n",
    "#                   target_out = torch.ones_like(mlp_out).to(mlp_out.device) # Match the output size \n",
    "#                   if printed_once: \n",
    "#                     # print(\"TARGET\", target_out) \n",
    "#                     printed_once = False\n",
    "\n",
    "#                 # Calculate the loss\n",
    "#                 loss = criterion(mlp_out, target_out)\n",
    "\n",
    "#                 # print(loss, loss.requires_grad)  # Should print a scalar tensor and True\n",
    "\n",
    "#                 # Backward pass to compute gradients w.r.t the perturbation\n",
    "#                 loss.backward()\n",
    "\n",
    "#                 # Access the gradient of the loss w.r.t. the perturbation\n",
    "#                 grad_input = perturbation.grad\n",
    "\n",
    "#                 # Define a learning rate\n",
    "#                 learning_rate = 0.01\n",
    "\n",
    "#                 # Update the perturbation in the direction of the negative gradient\n",
    "#                 perturbation = (perturbation - learning_rate * grad_input).clone().detach().requires_grad_().to(feature_vector.device)\n",
    "                \n",
    "#               ## check new predicted class\n",
    "#               # # pred = mlp(perturbation)\n",
    "#               # pred = mlp(perturbed_vector)\n",
    "\n",
    "#               # # Apply a sigmoid to convert it into a probability (between 0 and 1)\n",
    "#               # probability = torch.sigmoid(pred)\n",
    "\n",
    "#               # # Check which category it belongs to (class 0 or 1)\n",
    "#               # predicted_class = (probability > 0.5).float()\n",
    "\n",
    "#               # print(\"NEW Predicted class:\", predicted_class.item())\n",
    "              \n",
    "#               ### \n",
    "              \n",
    "              \n",
    "#               new_out = perturbed_vector, output[1] # concat perturbed vector with the rest of the output\n",
    "\n",
    "#               # returnn perturbation\n",
    "#               return new_out\n",
    "          \n",
    "#         return hook\n",
    "\n",
    "#     layers_to_enum = get_res_layers_to_enumerate(model)\n",
    "#     hooks = []\n",
    "\n",
    "#     # Register hooks for each specified layer\n",
    "#     for layer_index in layers_to_intervene:\n",
    "#         if layer_index < 0 or layer_index >= len(layers_to_enum):\n",
    "#             raise ValueError(f\"Layer {layer_index} is out of bounds for the model.\")\n",
    "#         hook_handle = model.model.layers[layer_index].register_forward_hook(linear_intervene(layer_index, target))\n",
    "#         hooks.append(hook_handle)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gen_data.get_activations import get_res_layers_to_enumerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lm_eval import evaluator\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "from torch.amp import autocast\n",
    "\n",
    "\n",
    "\n",
    "def insert_mlp_hook(model, \n",
    "                 mlp,\n",
    "                 layers_to_intervene=None,  # Allow multiple layers\n",
    "                 offensive = True, \n",
    "                 nr_perturbations = 34,\n",
    "                 learning_rate = 0.005,\n",
    "                 c = 0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if layers_to_intervene is None:\n",
    "        layers_to_intervene = [17]  # Default to layer 17 if no layers specified\n",
    "\n",
    "    # print out devices \n",
    "    # add print statements to see what we are stuck on \n",
    "    # try running function while disabling hook \n",
    "    criterion = nn.MSELoss()\n",
    "    def linear_intervene(name, c, offensive):\n",
    "        def hook(model, input, output):\n",
    "          \n",
    "          with torch.enable_grad():\n",
    "                      \n",
    "            # Forward pass\n",
    "            feature_vector = output[0][:, :, :].requires_grad_().to(output[0].device)          \n",
    "            # print(feature_vector.shape, feature_vector.requires_grad)  # Should print torch.Size([1, 768]) True\n",
    "            \n",
    "            ## check old predicted class\n",
    "            pred = mlp(feature_vector)\n",
    "            # print(\"OLD MLP_out:\", pred)\n",
    "            \n",
    "            ### \n",
    "          \n",
    "            \n",
    "                    \n",
    "            # Define perturbation as a parameter or tensor that requires gradient\n",
    "            perturbation = torch.zeros_like(feature_vector, requires_grad=True).to(feature_vector.device)\n",
    "            \n",
    "            printed_once = False\n",
    "            \n",
    "            for i in range(nr_perturbations):\n",
    "              # Apply perturbation to the feature vector\n",
    "              perturbed_vector = feature_vector + perturbation\n",
    "\n",
    "              # # Get output from the MLP\n",
    "              # if loss == \"BCE_new\":\n",
    "              #   mlp_out = sigmoid(mlp(perturbed_vector))\n",
    "              # else: \n",
    "              mlp_out = mlp(perturbed_vector)\n",
    "              \n",
    "              mlp_out_value = mlp_out.item()\n",
    "              \n",
    "              # target_out = torch.zeros_like(mlp_out)  # Default to zero, or choose an appropriate value\n",
    "              if offensive:\n",
    "                  # target = mlp_out_value + c\n",
    "                  target = max(mlp_out_value+c, c) # if out is very negative, target is c\n",
    "\n",
    "              else: \n",
    "                # defensive\n",
    "                target = min(mlp_out_value-c, -c) # if out is very positive, target is -c\n",
    "                    \n",
    "              \n",
    "              if not printed_once: \n",
    "                # print(\"target = \", target)\n",
    "                printed_once = True\n",
    "\n",
    "              target_out = torch.full_like(mlp_out, target)               \n",
    "\n",
    "              # Calculate the loss\n",
    "              loss = criterion(mlp_out, target_out).to(feature_vector.device)\n",
    "\n",
    "              # print(loss, loss.requires_grad)  # Should print a scalar tensor and True\n",
    "\n",
    "              # Backward pass to compute gradients w.r.t the perturbation\n",
    "              loss.backward()\n",
    "\n",
    "              # Access the gradient of the loss w.r.t. the perturbation\n",
    "              grad_input = perturbation.grad\n",
    "\n",
    "              # Define a learning rate\n",
    "              # learning_rate = 0.01\n",
    "\n",
    "              \n",
    "              perturbation = (perturbation - learning_rate * grad_input).clone().detach().requires_grad_().to(output[0].device)\n",
    "\n",
    "            ## check new predicted class\n",
    "            # pred = mlp(perturbation)\n",
    "            pred = mlp(perturbed_vector)\n",
    "            # print(\"NEW MLP_out:\", pred)\n",
    "            \n",
    "            ### \n",
    "            \n",
    "            # Clear unused variables\n",
    "            del grad_input, mlp_out, loss\n",
    "            torch.cuda.empty_cache()\n",
    "                    \n",
    "            new_out = perturbed_vector, output[1] # concat perturbed vector with the rest of the output\n",
    "\n",
    "            # returnn perturbation\n",
    "            return new_out\n",
    "          \n",
    "        return hook\n",
    "\n",
    "    layers_to_enum = get_res_layers_to_enumerate(model)\n",
    "    hooks = []\n",
    "\n",
    "    # Register hooks for each specified layer\n",
    "    for layer_index in layers_to_intervene:\n",
    "        if layer_index < 0 or layer_index >= len(layers_to_enum):\n",
    "            raise ValueError(f\"Layer {layer_index} is out of bounds for the model.\")\n",
    "        hook_handle = model.model.layers[layer_index].register_forward_hook(linear_intervene(layer_index, c, offensive))\n",
    "        hooks.append(hook_handle)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_mlp_hook(model, \n",
    "#                  mlp,\n",
    "#                  layers_to_intervene=None,  # Allow multiple layers\n",
    "#                  target = 0, \n",
    "#                  offensive = True, \n",
    "#                  nr_perturbations = 34,\n",
    "#                  learning_rate = 0.005):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     if layers_to_intervene is None:\n",
    "#         layers_to_intervene = [17]  # Default to layer 17 if no layers specified\n",
    "\n",
    "#     # print out devices \n",
    "#     # add print statements to see what we are stuck on \n",
    "#     # try running function while disabling hook \n",
    "#     criterion = nn.MSELoss()\n",
    "#     def linear_intervene(name, c, offensive):\n",
    "#         def hook(model, input, output):\n",
    "          \n",
    "#           with torch.enable_grad(), autocast():\n",
    "                      \n",
    "#             # Forward pass\n",
    "#             feature_vector = output[0][:, :, :].requires_grad_().to(output[0].device)          \n",
    "#             # print(feature_vector.shape, feature_vector.requires_grad)  # Should print torch.Size([1, 768]) True\n",
    "            \n",
    "#             ## check old predicted class\n",
    "#             pred = mlp(feature_vector)\n",
    "#             # print(\"OLD MLP_out:\", pred)\n",
    "            \n",
    "#             ### \n",
    "          \n",
    "            \n",
    "                    \n",
    "#             # Define perturbation as a parameter or tensor that requires gradient\n",
    "#             perturbation = torch.zeros_like(feature_vector, requires_grad=True).to(feature_vector.device)\n",
    "            \n",
    "#             printed_once = False\n",
    "            \n",
    "#             for i in range(nr_perturbations):\n",
    "#               # Apply perturbation to the feature vector\n",
    "#               perturbed_vector = feature_vector + perturbation\n",
    "\n",
    "#               # # Get output from the MLP\n",
    "#               # if loss == \"BCE_new\":\n",
    "#               #   mlp_out = sigmoid(mlp(perturbed_vector))\n",
    "#               # else: \n",
    "#               mlp_out = mlp(perturbed_vector)\n",
    "              \n",
    "#               mlp_out_value = mlp_out.item()\n",
    "              \n",
    "#               # target_out = torch.zeros_like(mlp_out)  # Default to zero, or choose an appropriate value\n",
    "#               if offensive == True:\n",
    "#                 if mlp_out_value >= 0:\n",
    "#                   # If mlp_out_value is positive, add c\n",
    "#                   target = mlp_out_value + c\n",
    "#                 else:\n",
    "#                     # If mlp_out_value is negative, ensure target is still larger than mlp_out_value\n",
    "#                     target = mlp_out_value + abs(mlp_out_value) + c\n",
    "                    \n",
    "#               else: # defensive\n",
    "#                 if mlp_out_value <= 0:\n",
    "#                   # If mlp_out_value is positive, add c\n",
    "#                   target = mlp_out_value - c\n",
    "#                 else:\n",
    "#                     # If mlp_out_value is negative, ensure target is still larger than mlp_out_value\n",
    "#                     target = mlp_out_value - abs(mlp_out_value) - c\n",
    "                    \n",
    "              \n",
    "#               if not printed_once: \n",
    "#                 # print(\"target = \", target)\n",
    "#                 printed_once = True\n",
    "\n",
    "#               target_out = torch.full_like(mlp_out, target)               \n",
    "\n",
    "#               # Calculate the loss\n",
    "#               loss = criterion(mlp_out, target_out).to(feature_vector.device)\n",
    "\n",
    "#               # print(loss, loss.requires_grad)  # Should print a scalar tensor and True\n",
    "\n",
    "#               # Backward pass to compute gradients w.r.t the perturbation\n",
    "#               loss.backward()\n",
    "\n",
    "#               # Access the gradient of the loss w.r.t. the perturbation\n",
    "#               grad_input = perturbation.grad\n",
    "\n",
    "#               # Define a learning rate\n",
    "#               # learning_rate = 0.01\n",
    "\n",
    "              \n",
    "#               perturbation = (perturbation - learning_rate * grad_input).clone().detach().requires_grad_().to(output[0].device)\n",
    "\n",
    "#             ## check new predicted class\n",
    "#             # pred = mlp(perturbation)\n",
    "#             pred = mlp(perturbed_vector)\n",
    "#             # print(\"NEW MLP_out:\", pred)\n",
    "            \n",
    "#             ### \n",
    "            \n",
    "#             # Clear unused variables\n",
    "#             del grad_input, mlp_out, loss\n",
    "#             torch.cuda.empty_cache()\n",
    "                    \n",
    "#             new_out = perturbed_vector, output[1] # concat perturbed vector with the rest of the output\n",
    "\n",
    "#             # returnn perturbation\n",
    "#             return new_out\n",
    "          \n",
    "#         return hook\n",
    "\n",
    "#     layers_to_enum = get_res_layers_to_enumerate(model)\n",
    "#     hooks = []\n",
    "\n",
    "#     # Register hooks for each specified layer\n",
    "#     for layer_index in layers_to_intervene:\n",
    "#         if layer_index < 0 or layer_index >= len(layers_to_enum):\n",
    "#             raise ValueError(f\"Layer {layer_index} is out of bounds for the model.\")\n",
    "#         hook_handle = model.model.layers[layer_index].register_forward_hook(linear_intervene(layer_index, target, offensive))\n",
    "#         hooks.append(hook_handle)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d6495c9a6c4bb6a55bfe354869d202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probe_path = \"/datasets/Probes/mlp_probe_weights.pth\"\n",
    "\n",
    "## CONFIG\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del variable\n",
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "\n",
    "def check_all_parameters(model):\n",
    "  return all(p.device.type == 'cuda' for p in model.parameters())\n",
    "\n",
    "hflm_model = HFLM(model)\n",
    "\n",
    "check_all_parameters(hflm_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from /data/nathalie_maria_kirch/ERA_Fellowship/experiments/nathaly/mlp_probe_weights.pth!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nathalie_maria_kirch/ERA_Fellowship/analysis/Probes.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(file_path))\n"
     ]
    }
   ],
   "source": [
    "from analysis.Probes import SimpleMLP\n",
    "mlp = SimpleMLP()\n",
    "\n",
    "# Load the model from the specified probe path\n",
    "mlp.load_model(probe_path)\n",
    "mlp.to(device)\n",
    "check_all_parameters(mlp)\n",
    "\n",
    "c = 10\n",
    "insert_mlp_hook(model=model, mlp=mlp, layers_to_intervene=[16], c = c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Model device check: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_hflm_model_devices(hflm_model):\n",
    "    print(\"HF Model device check:\", check_all_parameters(hflm_model.model))\n",
    "    for name, param in hflm_model.model.named_parameters():\n",
    "        # print(f\"{name} on {param.device}\")\n",
    "        if param.device.type != 'cuda':\n",
    "            # print(f\"Parameter {name} is not on GPU\")\n",
    "            return False\n",
    "    return True\n",
    "        \n",
    "check_hflm_model_devices(hflm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████| 216/216 [00:00<00:00, 851.71it/s]\n",
      "100%|███████████████| 203/203 [00:00<00:00, 854.92it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 855.47it/s]\n",
      "100%|███████████████| 152/152 [00:00<00:00, 857.34it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 851.04it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 849.99it/s]\n",
      "100%|███████████████| 145/145 [00:00<00:00, 858.32it/s]\n",
      "100%|███████████████| 151/151 [00:00<00:00, 864.39it/s]\n",
      "100%|███████████████| 112/112 [00:00<00:00, 853.17it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 849.75it/s]\n",
      "100%|███████████████| 135/135 [00:00<00:00, 857.20it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 846.88it/s]\n",
      "100%|███████████████| 144/144 [00:00<00:00, 854.79it/s]\n",
      "100%|███████████████| 310/310 [00:00<00:00, 856.62it/s]\n",
      "100%|███████████████| 235/235 [00:00<00:00, 854.43it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 851.41it/s]\n",
      "100%|███████████████| 270/270 [00:00<00:00, 859.38it/s]\n",
      "100%|███████████████| 102/102 [00:00<00:00, 854.29it/s]\n",
      "100%|███████████████| 378/378 [00:00<00:00, 856.01it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 849.34it/s]\n",
      "100%|███████████████| 234/234 [00:00<00:00, 861.82it/s]\n",
      "100%|███████████████| 306/306 [00:00<00:00, 859.10it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 853.34it/s]\n",
      "100%|███████████████| 265/265 [00:00<00:00, 851.81it/s]\n",
      "100%|███████████████| 103/103 [00:00<00:00, 850.49it/s]\n",
      "100%|███████████████| 783/783 [00:00<00:00, 857.85it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 849.64it/s]\n",
      "100%|███████████████| 223/223 [00:00<00:00, 859.94it/s]\n",
      "100%|███████████████| 272/272 [00:00<00:00, 855.22it/s]\n",
      "100%|███████████████| 166/166 [00:00<00:00, 864.08it/s]\n",
      "100%|███████████████| 282/282 [00:00<00:00, 853.29it/s]\n",
      "100%|███████████████| 173/173 [00:00<00:00, 852.27it/s]\n",
      "100%|███████████████| 110/110 [00:00<00:00, 850.38it/s]\n",
      "100%|███████████████| 201/201 [00:00<00:00, 856.26it/s]\n",
      "100%|███████████████| 193/193 [00:00<00:00, 855.70it/s]\n",
      "100%|███████████████| 612/612 [00:00<00:00, 850.85it/s]\n",
      "100%|███████████████| 545/545 [00:00<00:00, 857.31it/s]\n",
      "100%|███████████████| 114/114 [00:00<00:00, 849.33it/s]\n",
      "100%|███████████████| 198/198 [00:00<00:00, 852.99it/s]\n",
      "100%|███████████████| 245/245 [00:00<00:00, 849.65it/s]\n",
      "100%|███████████████| 100/100 [00:00<00:00, 849.16it/s]\n",
      "100%|███████████████| 131/131 [00:00<00:00, 857.63it/s]\n",
      "100%|███████████████| 238/238 [00:00<00:00, 860.59it/s]\n",
      "100%|███████████████| 390/390 [00:00<00:00, 860.25it/s]\n",
      "100%|███████████████| 895/895 [00:01<00:00, 857.41it/s]\n",
      "100%|███████████████| 237/237 [00:00<00:00, 859.49it/s]\n",
      "100%|███████████████| 108/108 [00:00<00:00, 848.79it/s]\n",
      "100%|███████████████| 346/346 [00:00<00:00, 859.90it/s]\n",
      "100%|███████████████| 204/204 [00:00<00:00, 859.58it/s]\n",
      "100%|███████████████| 324/324 [00:00<00:00, 856.40it/s]\n",
      "100%|███████████████| 163/163 [00:00<00:00, 862.76it/s]\n",
      "100%|███████████████| 171/171 [00:00<00:00, 857.09it/s]\n",
      "100%|███████████████| 121/121 [00:00<00:00, 854.37it/s]\n",
      "100%|█████████████| 1534/1534 [00:01<00:00, 855.80it/s]\n",
      "100%|███████████████| 165/165 [00:00<00:00, 852.47it/s]\n",
      "100%|███████████████| 126/126 [00:00<00:00, 851.00it/s]\n",
      "100%|███████████████| 311/311 [00:00<00:00, 862.47it/s]\n",
      "Running loglikelihood requests: 100%|█| 56168/56168 [42\n"
     ]
    }
   ],
   "source": [
    "hflm_model.model.to(device)\n",
    "eval_output = evaluator.simple_evaluate(model=hflm_model, tasks=['mmlu'], device='cuda', verbosity='ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval_output['results']['mmlu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc,none': 0.5068366329582681,\n",
       " 'acc_stderr,none': 0.0040292234599421314,\n",
       " 'alias': 'mmlu'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "file_path = '/datasets/capabilities/capabilities.json'\n",
    "\n",
    "# Convert the numeric key to a string\n",
    "key_string = f\"mlp {str(c)}\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        existing_data = json.load(json_file)\n",
    "        \n",
    "        # If the key already exists, append the result to the list under that key\n",
    "        if key_string in existing_data:\n",
    "            existing_data[key_string].append(result)\n",
    "        else:\n",
    "            existing_data[key_string] = [result]  # Start a new list if the key doesn't exist\n",
    "else:\n",
    "    existing_data = {key_string: [result]}  # Create a new dict with the key and a list containing the result\n",
    "\n",
    "# Write back to the JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(existing_data, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
